---
title: "topic_modelling"
output: html_document
date: "2024-05-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load and clean data

```{r}
library(tidyverse)
```

```{r cars}
old_beauty <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/hackafun_cosmetic_joined.csv")

# beauty_embeddings <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_embeddings.rds")

beauty <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/hackafun_cosmetic_joined.csv") %>%
  dplyr::rename(text_clean = text_copy)


beauty_reduced_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_reduced_embeddings.csv") %>%
  dplyr::select(-1)
  
```

```{r}
#' clean_text
#' @description A function that performs a series of cleaning steps on a text variable. Useful for processing when dealing with qualitative data
#' @param df A tibble or data frame object containing the text variable the user wants to perform cleaning steps upon
#' @param text_var The text variable with the message assigned to the observation that the user wishes to clean
#' @param tolower Whether to convert all text to lower case?
#' @param remove_hashtags Should hashtags be removed?
#' @param remove_mentions Should any user/profile mentions be removed?
#' @param remove_emojis Should emojis be removed?
#' @param remove_punctuation Should punctuation be removed?
#' @param remove_digits Should digits be removed?
#' @param in_parallel Whether to run the function in parallel (TRUE = faster)
#'
#' @return The data object provided, with a cleaned text variable
#'
#' @export
#'
#' @examples
#' if(interactive()){
#' #Performs all cleaning steps in parallel
#'cleaned_data <- clean_text(df = ParseR::sprinklr_export,
#'text_var = Message,
#'in_parallel = TRUE)
#'
#'# If the user wants to perform all cleaning steps but keep capital letters and punctuation 
#'cleaned_data <- clean_text(df = ParseR::sprinklr_export,
#'text_var = Message,
#'tolower = FALSE,
#'remove_punctuation = FALSE,
#'in_parallel = TRUE)
#' }

clean_text_ar <- function(df, text_var = message, tolower = TRUE, remove_hashtags = TRUE, remove_mentions = TRUE, remove_emojis = TRUE, remove_punctuation = TRUE, remove_digits = TRUE, in_parallel = TRUE) {
  text_sym <- rlang::ensym(text_var)
  text_quo <- rlang::enquo(text_var)
  
  # hide regex ----
  # Non-optional regex for websites
  domains <- c(".com", ".ly", ".org", ".net", ".us", ".uk", ".co", ".ch")
  http_regex <- "htt(p|ps)\\S+"
  web_regex <- paste0(
    "[:graph:]*(?=(\\", domains, "/))",
    "|(?<=(\\", domains, "/))[:graph:]*(?![:alnum:])"
  )
  domain_regex <- paste0("(\\", domains, "/)")
  
  # Optional regex for hashtags
  if (remove_hashtags) {
    hashtags_regex <- c("(?<=#)[:graph:]*(?![:graph:])|(?<=#)[:graph:]*$", "#")
  } else {
    hashtags_regex <- NULL
  }
  
  # Optional regex for hashtags
  if (remove_mentions) {
    mentions_regex <- c("(?<=@)[:graph:]*(?![:graph:])|(?<=@)[:graph:]*$", "@")
  } else {
    mentions_regex <- NULL
  }
  
  # Optional regex for emojis
  if (remove_emojis) {
    emojis_regex <- "[^\x01-\x7F]"
  } else {
    emojis_regex <- NULL
  }
  if (remove_punctuation) {
    punctuation_regex <- "[:punct:]"
  } else {
    punctuation_regex <- NULL
  }
  
  if (remove_digits) {
    digits_regex <- "[:digit:]"
  } else {
    digits_regex <- NULL
  }
  
  # Join all regex into one named character vector
  names_regex <- c(
    web_regex,
    domain_regex,
    hashtags_regex,
    mentions_regex,
    digits_regex,
    emojis_regex,
    punctuation_regex,
    http_regex
  )
  all_regex <- character(length(names_regex))
  names(all_regex) <- names_regex
  
  # hide function main body ----
  
  if (tolower) {
    df <- df %>%
      dplyr::mutate(!!text_quo := tolower(!!text_sym))
  }
  
  if (in_parallel) {
    num_cuts <- future::availableCores() - 1
    
    options(future.rng.onMisuse = "ignore")
    message("Beginning parallel sessions")
    future::plan(future::multisession(workers = future::availableCores() - 1))
    
    df <- df %>%
      dplyr::mutate(.document = dplyr::row_number()) %>%
      dplyr::group_split(.document %% num_cuts) %>%
      furrr::future_map_dfr(~ .x %>%
                              dplyr::mutate(!!text_sym := stringr::str_remove_all(!!text_sym, all_regex),
                                            !!text_sym := stringr::str_squish(!!text_sym),
                                            !!text_sym := stringr::str_trim(!!text_sym))) %>%
      dplyr::arrange(.document) %>%
      dplyr::select(-.document)
    
    message("Ending parallel sessions")
    future::plan(future::sequential())
  } else {
    df <- df %>%
      dplyr::mutate(
        !!text_sym := stringr::str_remove_all(!!text_sym, all_regex),
        !!text_sym := stringr::str_squish(!!text_sym)
      ) 
  }
  
  df <- df %>% dplyr::filter(!is.na(!!text_sym))
  return(df)
}

```

DON'T USE THIS!!!! This all quick work I did for the demo - should use Tim's dataset not this
```{r}
# mentions_regex <- c("(?<=@)[:graph:]*(?![:graph:])|(?<=@)[:graph:]*$", "@")
mentions_regex <- "@[a-z,A-Z]*?"

beauty_clean <- old_beauty %>%
  dplyr::mutate(text_clean = text) %>%
  clean_text_ar(text_var = text_clean, 
                tolower = FALSE, 
                remove_hashtags = TRUE, 
                remove_mentions = TRUE, 
                remove_emojis = TRUE, 
                remove_punctuation = FALSE, 
                remove_digits = FALSE, 
                in_parallel = TRUE) %>%
  dplyr::mutate(char_length = stringr::str_count(text_clean)) %>%
  dplyr::filter(char_length > 10) %>%
  dplyr::distinct(text_clean, .keep_all = TRUE)

beauty_clean %>% head(100) %>%
  dplyr::select(text, text_copy, text_clean) %>% DT::datatable()


beauty_clean %>% readr::write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/old/cosmetic_joined_rev2.csv")

beauty_clean <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/old/cosmetic_joined_rev2.csv")

reduced_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_joined_reduced_embeddings_ar.csv")

reduced_embeddings2d <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_joined_reduced_embeddings2d_ar.csv")
```

## Quick Topic Modelling for Demo

```{r}
library(BertopicR)

clusterer <- bt_make_clusterer_kmeans(n_clusters = 10L)

model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = clusterer)
bt_fit_model(model, 
             beauty_clean$text_clean, 
             embeddings = reduced_embeddings)

model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


representation <- bt_representation_openai(fitted_model = model,
                         documents = beauty_clean$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)

topic_summary <- model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(representation = representation)

topic_lookup <- topic_summary %>% 
  dplyr::select(Topic, representation)

beauty_topics <- beauty_clean %>%
  dplyr::mutate(topic = model$topics_,
                text_with_breaks = sapply(text, insert_line_breaks)) %>%
  dplyr::left_join(topic_lookup, 
                   by = dplyr::join_by("topic" == "Topic")) %>%
  dplyr::rename(topic_title = representation) %>%
  dplyr::select(-c(".document%%num_cuts", "V1", "V2")) %>%
  cbind(reduced_embeddings2d) %>% 
  dplyr::rename(V1 = `0`, V2 = `1`) %>%
  dplyr::mutate(rowid = dplyr::row_number(),
                topic_title = dplyr::case_when(
                  topic == 0 ~ "Hair Care",
                  topic == 1 ~ "Moisturising\Skincare",
                  topic == 2 ~ "Perfumes",
                  topic == 3 ~ "Social Issues",
                  topic == 4 ~ "Colognes",
                  topic == 5 ~ "Skincare for\nAcne/Wrinkles",
                  topic == 6 ~ "Skin Conditions",
                  topic == 7 ~ "Feeling Radiant",
                  topic == 8 ~ "Designers",
                  topic == 9 ~ "Makeup",
                  TRUE ~ NA))

beauty_topics %>%
  readr::write_rds("app/data/cosmetic_data.rds")
```

# Topic Modelling to remove spam
```{r}
library(BertopicR)

clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 100L, 
                                       min_samples = 50L,
                                       cluster_selection_method = "leaf")

model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = clusterer)
bt_fit_model(model, 
             beauty$text_clean, 
             embeddings = beauty_reduced_embeddings)

model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


representation <- bt_representation_openai(fitted_model = model,
                         documents = beauty$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)

topic_summary <- model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(openai_representation = representation)

topic_summary %>% DT::datatable()

beauty %>%
  mutate(topic = model$topics_) %>%
  filter(topic == 50) %>%
  sample_n(100) %>% select(text, text_clean) %>% DT::datatable()
```

The following are clusters that need to be removed:
 c(1, 3, 11, 13, 15, 23, 24, 25, 27, 28, 31, 35, 37, 
 38, 39, 50, 55, 59, 62, 69, 71, 75, 77, 79, 82,  84, 86, 
 87, 92, 94, 95, 96, 97)

The following are interesting clusters:
- 72: Charlotte Tilbury X F1
- Don't remember the numbers but Becky X Maybelline and human Dior

Before actually removing the topics I will perform outlier reduction

```{r}
beauty_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_BAAI_normalised_embeddings.csv") %>%
  dplyr::select(-1)

embedder <- bt_make_embedder_st("BAAI/bge-large-en-v1.5")
outliers <- bt_outliers_embeddings(fitted_model = model,
                                   documents = beauty$text_clean,
                                   topic = model$topics_,
                                   embeddings = beauty_embeddings,
                                   embedding_model = embedder,
                                   threshold = 0.1)

outliers %>% 
  filter(current_topics == -1, new_topics == 49) %>% 
  select(message) %>% DT::datatable()

remove_clusters <- c(1, 3, 11, 13, 15, 23, 24, 25, 27, 28, 31, 35, 37, 
 38, 39, 50, 55, 59, 62, 69, 71, 75, 77, 79, 82,  84, 86, 
 87, 92, 94, 95, 96, 97)

beauty_hdb <- beauty %>%
  mutate(outlier_reduced_topic = outliers$new_topics,
         hdb_topic = model$topics_) %>%
  # mutate(hdb_topic = model$topics_) %>%
  # filter(!hdb_topic %in% remove_clusters)
  filter(!outlier_reduced_topic %in% remove_clusters)


hdb_topic_lookup <-  topic_summary %>% 
  select(Topic, hdb_topic_title = openai_representation)

beauty_hdb %>%
  left_join(hdb_topic_lookup, by = join_by("hdb_topic" == "Topic")) %>% 
  select(hdb_topic, openai_representation) 


# previous save
# beauty_hdb %>%
#   readr::write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics.csv")

beauty_hdb %>%
  readr::write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics_including_before_outlier_reduction_4jan2024.csv")

```

Now let's do some kmeans - first - do I need to rembed?
```{r}
kept_rows <- which(!outliers$new_topics %in% remove_clusters)
beauty_embeddings_subset <- beauty_embeddings[kept_rows,]
beauty_hdb_embeddings <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics_embeddings.csv") %>%
  dplyr::select(-1)

round(beauty_hdb_embeddings[1:50, 1:10], 5) == round(beauty_embeddings_subset[1:50, 1:10], 5)
```

Conclusion - not exactly the same but close enough that I don't think we need to bother - we would need to re-reduce though. I will do this on colab witht the recalculated embeddings seen as we already have them.

```{r}
beauty_reduced_embeddings_hdb <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_reduced_hdb_embeddings.csv")


kmeans_clusterer <- bt_make_clusterer_kmeans(n_clusters = 10L)
kmeans_clusters <- bt_do_clustering(kmeans_clusterer, beauty_reduced_embeddings_hdb)

cont_tab <- table(kmeans_clusters, beauty_hdb$hdb_topic)
```


Looks like the hdbscan clusters fall fairly neatly into the kmeans ones. Let's save the results.
```{r}
kmeans_model <- bt_compile_model(embedding_model = bt_empty_embedder(),
                          reduction_model = bt_empty_reducer(),
                          clustering_model = kmeans_clusterer)
bt_fit_model(kmeans_model, 
             beauty_hdb$text_clean, 
             embeddings = beauty_reduced_embeddings_hdb)

kmeans_model$get_topic_info() %>% 
  dplyr::select(-Representative_Docs, - Representation) %>%
  DT::datatable()


kmeans_representation <- bt_representation_openai(fitted_model = kmeans_model,
                         documents = beauty_hdb$text_clean,
                         openai_model = "gpt-3.5-turbo",
                         api_key = Sys.getenv("OPENAI_API_KEY"),
                         nr_repr_docs = 150L,
                         nr_sample = 1000L,
                         chat = TRUE)


kmeans_topic_summary <- kmeans_model$get_topic_info() %>%
  dplyr::select(-Representative_Docs, - Representation) %>%
  dplyr::mutate(openai_representation = kmeans_representation)

kmeans_topic_summary %>% DT::datatable()

kmeans_topic_lookup <- kmeans_topic_summary %>%
  select(Topic, kmeans_topic_title = openai_representation)

insert_line_breaks <- function(text, n = 10) {
  words <- strsplit(text, " ")[[1]]
  paste(sapply(seq(1, length(words), n), function(i) {
    paste(words[i:min(i + n - 1, length(words))], collapse = " ")
  }), collapse = "<br>")
}

```


Now add recalculated V1 and V2 from colab and save
```{r}
# embeddings2d <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_reduced_hdb_embeddings2d_rev2.csv")

# I really don't like these reduced embeddings
reducer <- BertopicR::bt_make_reducer_umap(n_components = 2L, metric = "cosine")
embeddings2d <- bt_do_reducing(reducer, beauty_hdb_embeddings)


beauty_topics <- beauty_hdb %>%
  dplyr::select(-c(V1, V2)) %>%
  mutate(text_with_breaks = sapply(text, insert_line_breaks),
         kmeans_topic = kmeans_model$topics_,
         V1 = embeddings2d[,1],
         V2 = embeddings2d[,2]) %>%
  left_join(kmeans_topic_lookup, by = join_by("kmeans_topic" == "Topic")) %>%
  left_join(hdb_topic_lookup, by = join_by("hdb_topic" == "Topic")) %>%
  mutate(hdb_topic_title = unlist(hdb_topic_title),
         keans_topic_title = unlist(kmeans_topic_title)) 

beauty_topics %>%
  write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_kmeans_and_hdb_topics.csv")


cosmetic_adj <- cosmetic_data %>%
  dplyr::select(-c(V1, V2)) %>%
  dplyr::mutate(V1 = unlist(embeddings2d[,1]),
         V2 = unlist(embeddings2d[,2]))

cosmetic_adj %>%
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds")
```

Now I should maybe tokenise

```{r}
install.packages("tidytext")
beauty_sentences <- tidytext::unnest_tokens(
  beauty_topics,
  output = sentences,
  input = text_clean,
  token = "sentences",
  to_lower = FALSE,
  drop = FALSE
  )

beauty_sentences %>%
  write_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_sentences.csv")
```

save sentence embeddings as rds
```{r}
cosmetic_sentence_embeddings <- readr::read_csv(
  "~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_sentences_embeddings.csv"
)

cosmetic_sentence_embeddings %>% 
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences_embeddings.rds")
```

# fixing external data labels
Fixing topic titles in sentence df (saved as list and so not present when reuploaded)
```{r}
cosmetic_data <- readr::read_rds(here::here("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds"))

cosmetic_sentences <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")

cosmetic_topic_info <- cosmetic_data %>%
  dplyr::select(kmeans_topic, kmeans_topic_title, hdb_topic, hdb_topic_title, universal_message_id)

cosmetic_sentence_topics <- cosmetic_sentences %>%
  select(-c(kmeans_topic, kmeans_topic_title, hdb_topic, hdb_topic_title)) %>%
  dplyr::left_join(cosmetic_topic_info, by = "universal_message_id")

# quick check
cosmetic_sentence_topics %>%
  distinct(kmeans_topic, kmeans_topic_title)

cosmetic_sentence_topics %>%
  distinct(hdb_topic, hdb_topic_title)

nrow(cosmetic_sentence_topics) == nrow(cosmetic_sentences)

cosmetic_data %>%
  distinct(kmeans_topic, kmeans_topic_title)

cosmetic_data %>%
  distinct(hdb_topic, hdb_topic_title)

cosmetic_sentence_topics %>% readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")
```

Shortening topic titles
```{r}
cosmetic_data %>% 
  distinct(kmeans_topic_title, kmeans_topic) %>%
  arrange(kmeans_topic)

cosmetic_data <- cosmetic_data %>%
    dplyr::mutate(kmeans_topic_title = dplyr::case_when(
      kmeans_topic == 0 ~ "Hair",
      kmeans_topic == 1 ~ "Fragrances",
      kmeans_topic == 2 ~ "Skincare",
      kmeans_topic == 3 ~ "Skin Concerns",
      kmeans_topic == 4 ~ "Sun Protection",
      kmeans_topic == 5 ~ "Designers",
      kmeans_topic == 6 ~ "Makeup",
      kmeans_topic == 7 ~ "Vaseline",
      kmeans_topic == 8 ~ "Body Shop",
      kmeans_topic == 9 ~ "Becky X Maybelline\nColab",
      TRUE ~ NA
    ))

cosmetic_data %>%
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds")
```


Ok now that I've shortened the cosmetic data and updated the V1 and V2 coordinates and kmeans topic titles, I need to update the sentence topic title
```{r}
cosmetic_data <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds")

cosmetic_sentences <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")

cosmetic_topic_info <- cosmetic_data %>%
  dplyr::select(kmeans_topic_title, kmeans_topic, V1, V2, universal_message_id)

cosmetic_sentence_topics <- cosmetic_sentences %>%
  dplyr::select(-c(kmeans_topic_title, V1, V2)) %>%
  dplyr::left_join(cosmetic_topic_info, by = c("universal_message_id", "kmeans_topic"))

# quick check
cosmetic_sentence_topics %>%
  dplyr::distinct(kmeans_topic, kmeans_topic_title) %>% dplyr::arrange(kmeans_topic)

cosmetic_sentence_topics %>%
  dplyr::distinct(kmeans_topic, kmeans_topic_title) %>% dplyr::arrange(kmeans_topic)

cosmetic_sentence_topics %>%
  dplyr::distinct(universal_message_id, .keep_all = TRUE) %>%
  dplyr::group_by(kmeans_topic, kmeans_topic_title) %>%
  dplyr::summarise(n = dplyr::n())

cosmetic_topic_info %>%
  dplyr::group_by(kmeans_topic, kmeans_topic_title) %>%
  dplyr::summarise(n = dplyr::n())


nrow(cosmetic_sentence_topics) == nrow(cosmetic_sentences)

cosmetic_sentence_topics %>% readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_sentences.rds")
```


Examining food and automotive columns
```{r}
automotive_data <- automotive_data %>% 
  dplyr::mutate(text_with_breaks = sapply(text, insert_line_breaks)) %>%
  dplyr::rename(hdb_topic = hdbscan_topic,
                hdb_topic_title = hdbscan_topic_title,
                text_clean = text_copy)

automotive_data_adj %>%
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/automotive_df.rds")

food_data  <-
  readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/old/food_beverage_df.rds")

food_data %>% colnames()

food_data_adj <- food_data %>% 
  dplyr::mutate(text_with_breaks = sapply(text, insert_line_breaks)) %>%
  rename(text_clean = text_copy)

food_data_adj %>%
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/food_beverage_df.rds")


# food_data %>%
#   mutate(kmeans_topic_title = dplyr::case_when(
#     topic_title == "Food Recipes & Menus" ~ ""
#   ))
```


# Trying different params to reduce automotive and food data to 2d
```{r}

```


# fixing topic labels
```{r}
automotive_data_adj <- automotive_data %>%
  dplyr::mutate(kmeans_topic_title = dplyr::case_when(
    kmeans_topic_title == "Modern Technologies\n& Advancements" ~
      "Modern\nTechnologies &\nAdvancements",
    kmeans_topic_title == "Sustainable Motoring" ~
      "Sustainable\nMotoring",
    TRUE ~ kmeans_topic_title
  ))


automotive_data_adj %>%
  readr::write_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/automotive_df.rds")
```

# Further topic modelling messing on cosmetic data
```{r}
library(dplyr)
cosmetic_data_test <- readr::read_rds("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/for_app/cosmetic_df.rds")


createUmap(df = cosmetic_data_test, cluster_type = "kmeans")

cosmetic_data_test %>%
  dplyr::group_by(hdb_topic_title, hdb_topic) %>%
  dplyr::summarise(n = n()) %>% arrange(desc(n)) %>% DT::datatable()

cosmetic_data %>% 
  filter(hdb_topic == 93) %>% 
  select(text, permalink, kmeans_topic_title) %>% DT::datatable()


cosmetic_data %>%
  plotly::plot_ly(x = ~V1, y = ~V2, type = "scatter", mode = "markers", color = ~as.factor(hdb_topic))
```

what about topics pre reducing outliers:

```{r}
before_outlier_reduction <- readr::read_csv("~/Google Drive/My Drive/Share_Clients/data_science_project_work/hackafun/data/cleaned_data/cosmetic_data/cosmetic_hdb_topics_including_before_outlier_reduction_4jan2024.csv")

pre_outlier_reduction_topics <- before_outlier_reduction %>%
  select(universal_message_id, og_hdb_topic = hdb_topic, hdb_topic = outlier_reduced_topic)

cosmetic_data_plot <- cosmetic_data_test %>%
  left_join(pre_outlier_reduction_topics) %>%
  mutate(og_hdb_topic_title = case_when(
    og_hdb_topic != -1 ~ hdb_topic_title,
    TRUE ~ "outlier"
  ))

topics <- unique(cosmetic_data_plot$og_hdb_topic)

test <- cosmetic_data_plot %>%
  mutate(og_hdb_topic = forcats::fct_reorder(as.factor(og_hdb_topic), og_hdb_topic)) 

test %>%  plotly::plot_ly(x = ~V1, y = ~V2, 
                  type = "scatter", mode = "markers", 
                  color = ~og_hdb_topic,
                  colors = c("#cccccc", 
                             viridis::viridis(
                               n = length(topics),
                               begin = 0.3, end = 0.92,
                               option = "B", direction = 1
                               )),
                  name = ~og_hdb_topic_title,
                  marker = list(opacity = 0.4, size = 4))
  

# beauty_hdb %>%
#   plotly::plot_ly(x = ~V1, y = ~V2, type = "scatter", mode = "markers", color = ~as.factor(hdb_topic))
```

# try to tidy up hdbscan cluster titles
```{r}
custom_prompt = "
I have a topic that contains the following documents: 
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]

Based on the information above, extract a short topic title in the following format:
topic: <topic title>

The topic title should be a max of 4 words.
"
openai_hdb_representation <- bt_representation_openai(
  fitted_model = model,
   documents = beauty$text_clean,
   openai_model = "gpt-3.5-turbo",
   api_key = Sys.getenv("OPENAI_API_KEY"),
   nr_repr_docs = 150L,
   nr_sample = 1000L,
   chat = TRUE,
  prompt = custom_prompt
)


```

```{r}
test_file <- googledrive::drive_get("for_app/food_beverage_sentences.rds")
test_temp_file <- tempfile(fileext = ".rds")
googledrive::drive_download(test_file, path = test_temp_file, overwrite = TRUE)
test_sentences <- readRDS(test_temp_file)
test %>% distinct(kmeans_topic_title)
colnames(test_sentences)
```



